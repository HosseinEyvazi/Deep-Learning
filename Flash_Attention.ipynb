{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "wLpi3lKp5EQt"
      ],
      "authorship_tag": "ABX9TyP/0UULOBK/TOrLnCwfIvZD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HosseinEyvazi/Deep-Learning/blob/main/Flash_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### **FlashAttention: The Complete Technical Booklet**\n",
        "**_Table of Contents_**\n",
        "\n",
        "**1. __The Hardware Foundation__**\n",
        "*   _1.1 The Two-Memory System (SRAM vs. HBM)_\n",
        "*   _1.2 The Golden Rule of GPU Computing_\n",
        "\n",
        "**2. __Why Standard Attention Creates $O(N^2)$ Space__**\n",
        "*   _2.1 The Attention Mechanism_\n",
        "*   _2.2 The “Every Token Talks to Every Token” Problem_\n",
        "*   _2.3 The Space Bottleneck_\n",
        "\n",
        "**3. __Standard Attention’s “Forced” Data Movement__**\n",
        "*   _3.1 The Loop of Forced I/O (Tiled Implementation)_\n",
        "\n",
        "**4. __FlashAttention’s Solution__**\n",
        "*   _4.1 Core Insight (Computation vs. Bandwidth)_\n",
        "*   _4.2 Three Pillars (Kernel Fusion, Online Softmax, Recomputation)_\n",
        "\n",
        "**5. __FlashAttention’s Memory Structure__**\n",
        "*   _5.1 Chunk / Tile Dimensions_\n",
        "*   _5.2 Why FlashAttention is $O(N)$ Space_\n",
        "*   _5.3 Comparison: Standard vs. FlashAttention_\n",
        "\n",
        "**6. __Mathematical Proof: Why Accumulation Works__**\n",
        "*   _6.1 Softmax and Output Equations_\n",
        "*   _6.2 Partial / Block Processing Problem_\n",
        "*   _6.3 Online Softmax with Rescaling_\n",
        "*   _6.4 Proof of Correctness_\n",
        "\n",
        "**7. __Numerical Stability: Why We Subtract the Max__**\n",
        "*   _7.1 Overflow Risk in Float32_\n",
        "*   _7.2 Stable Softmax by Subtracting the Max_\n",
        "\n",
        "**8. __Time Complexity vs. Runtime__**\n",
        "*   _8.1 Same FLOPs, Different Runtime_\n",
        "*   _8.2 Runtime is Dominated by Data Movement_\n",
        "*   _8.3 Practical Physics_\n",
        "\n",
        "**9. __On-Chip Memory (SRAM) Explained__**\n",
        "*   _9.1 Physical Reality_\n",
        "*   _9.2 Why SRAM is Small_\n",
        "*   _9.3 Cache / Memory Hierarchy_\n",
        "\n",
        "**10. __Summary Table__**\n",
        "\n",
        "**11. __Final Takeaways__**\n",
        "\n",
        "**__Appendix: Key Formulas at a Glance__**\n",
        "\n",
        "***\n",
        "\n",
        "### **Booklet Structure Summary**\n",
        "\n",
        "Here is the logic flow table to help you scan the document's critical path. Use this to apply **Critical Thinking** to how the hardware constraints dictate the algorithmic solution .\n",
        "\n",
        "| **Section Block** | **Chapters** | **Core Concept (Focus)** | **Key Takeaway for Research** |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **The Problem** | Ch 1–3 | **Hardware Bottlenecks** | The GPU is limited by HBM bandwidth (Memory Wall), not compute power. Standard Attention wastes bandwidth by moving $N^2$ matrices . |\n",
        "| **The Solution** | Ch 4–5 | **Algorithmic Optimization** | **Tiling & Fusion:** Keep data in fast SRAM. Only write the final result to HBM. Complexity drops from $O(N^2)$ Memory to $O(N)$ . |\n",
        "| **The Proof** | Ch 6–7 | **Mathematical Validity** | **Online Softmax:** You can compute Softmax incrementally (block-by-block) without seeing the full row at once, using rescaling factors . |\n",
        "| **The Impact** | Ch 8–11 | **Performance Reality** | Even with re-computation (more FLOPs in backward pass), the speedup is significant because we avoid the slow HBM transfers . |"
      ],
      "metadata": {
        "id": "7D8lEhUos_ts"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FlashAttention: The Complete Technical Booklet\n",
        "\n",
        "## From Memory Bottleneck to Hardware-Aware Algorithm\n",
        "\n",
        "*Author’s Note:* This booklet is designed for Data Scientists and Systems Engineers who need to understand **why** FlashAttention is revolutionary, not just that it exists.\n",
        "\n",
        "---\n",
        "\n",
        "## Chapter 1 — The Hardware Foundation\n",
        "\n",
        "### 1.1 The Two-Memory System\n",
        "\n",
        "Your GPU is fundamentally split into two memory systems.\n",
        "\n",
        "**SRAM (On-Chip Memory)**\n",
        "\n",
        "* **Location:** Inside the silicon of the Compute Unit (Streaming Multiprocessor).\n",
        "* **Size:** ~100 KB – 20 MB per SM (implementation dependent).\n",
        "* **Speed:** Very high bandwidth (the \"fast lane\").\n",
        "* **Latency:** Nanoseconds.\n",
        "* **Analogy:** Your brain's working memory — what you are actively thinking about.\n",
        "\n",
        "**HBM (High-Bandwidth Memory — Off-Chip)**\n",
        "\n",
        "* **Location:** Outside the processor die, soldered near the GPU chip.\n",
        "* **Size:** Tens of gigabytes (e.g., 40–80 GB).\n",
        "* **Speed:** Lower bandwidth vs SRAM (e.g., ~1.5–3 TB/s on modern GPUs).\n",
        "* **Latency:** Microseconds (slower).\n",
        "* **Analogy:** Your library — large storage, but you must walk to fetch books.\n",
        "\n",
        "### 1.2 The Golden Rule of GPU Computing\n",
        "\n",
        "**The GPU compute units can only operate on data that is present in on-chip SRAM.**\n",
        "\n",
        "Every byte must follow this journey:\n",
        "\n",
        "1. Read from HBM into SRAM.\n",
        "2. Compute in SRAM (or registers).\n",
        "3. Write results back to HBM.\n",
        "\n",
        "**Cost analysis (illustrative):**\n",
        "\n",
        "* Moving 1 byte (HBM ↔ SRAM): large number of cycles (orders of magnitude more than an arithmetic op).\n",
        "* Computing on 1 byte: a few cycles.\n",
        "\n",
        "**Conclusion:** Data movement is far more expensive than arithmetic. Minimize data movement.\n",
        "\n",
        "---\n",
        "\n",
        "## Chapter 2 — Why Standard Attention Creates $O(N^2)$ Space\n",
        "\n",
        "### 2.1 The Attention Mechanism\n",
        "\n",
        "Scaled dot-product attention is:\n",
        "\n",
        "$$\n",
        "\\text{Attention}(Q,K,V) ;=; \\text{Softmax}!\\Big(\\frac{QK^\\top}{\\sqrt{d_k}}\\Big);V.\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $Q$ is the query matrix ($N\\times d$).\n",
        "* $K$ is the key matrix ($N\\times d$).\n",
        "* $V$ is the value matrix ($N\\times d$).\n",
        "* $N$ is the sequence length.\n",
        "* $d$ is the head dimension.\n",
        "\n",
        "Computing $QK^\\top$ produces an $N\\times N$ score matrix of pairwise similarities.\n",
        "\n",
        "### 2.2 The “Every Token Talks to Every Token” Problem\n",
        "\n",
        "Attention computes a score between every pair of tokens. For $N$ tokens there are $N^2$ interactions.\n",
        "\n",
        "Example, $N=4$ (A, B, C, D) — 16 scores.\n",
        "\n",
        "Scale examples:\n",
        "\n",
        "* $N=2{,}048$: $2{,}048^2 \\approx 4{,}194{,}304$ scores (≈16 MB in 4-byte floats).\n",
        "* $N=32{,}000$: $32{,}000^2 \\approx 1{,}024{,}000{,}000$ scores (≈4 GB).\n",
        "* $N=100{,}000$: $100{,}000^2 = 10^{10}$ scores (≈40 GB).\n",
        "\n",
        "### 2.3 The Space Bottleneck\n",
        "\n",
        "Standard attention typically **materializes** the full $N\\times N$ score matrix (and often the probability matrix) in memory. For very long contexts this becomes infeasible — e.g., a single 100K × 100K score matrix at 4 bytes per entry is ~40 GB, leaving almost no GPU memory for model weights or gradients.\n",
        "\n",
        "Thus standard attention requires $O(N^2)$ memory, which does not scale to extremely long sequences.\n",
        "\n",
        "---\n",
        "\n",
        "## Chapter 3 — Standard Attention’s “Forced” Data Movement\n",
        "\n",
        "### 3.1 The Loop of Forced I/O (Tiled Implementation)\n",
        "\n",
        "Even with tiling (block size $B$), a naive implementation forces many HBM transfers:\n",
        "\n",
        "1. **Load** a block of $Q$ and a block of $K$ from HBM to SRAM.\n",
        "2. **Compute** $S_{\\text{block}} = Q_{\\text{block}} K_{\\text{block}}^\\top$ in SRAM (size $B\\times B$).\n",
        "3. **Write** that partial score block to HBM (evict from SRAM). (bottleneck)\n",
        "4. Repeat until the entire $N\\times N$ $S$ matrix is assembled in HBM.\n",
        "5. To compute row-wise Softmax you must **read** the row’s $N$ scores back into SRAM.\n",
        "6. Compute probabilities and **write** the $P$ blocks to HBM.\n",
        "7. Read $P$ and $V$ to compute final output $O = PV$.\n",
        "\n",
        "**Total I/O complexity:** $O(N^2)$ reads/writes. The GPU spends a large fraction of time on memory traffic instead of arithmetic.\n",
        "\n",
        "(Concrete numeric example in the naive narrative: writing/reading multi-gigabyte intermediates for large $N$.)\n",
        "\n",
        "---\n",
        "\n",
        "## Chapter 4 — FlashAttention’s Solution\n",
        "\n",
        "### 4.1 Core Insight\n",
        "\n",
        "FlashAttention keeps intermediate results on-chip and fuses operations so that the algorithm:\n",
        "\n",
        "* Performs the same arithmetic (same FLOPs), **but**\n",
        "* Minimizes HBM traffic by never materializing the full $N\\times N$ intermediate matrices.\n",
        "\n",
        "Key principle: computation is cheap relative to bandwidth; trade extra arithmetic or recomputation for fewer memory accesses.\n",
        "\n",
        "### 4.2 Three Pillars\n",
        "\n",
        "**Pillar 1 — Kernel fusion**\n",
        "Fuse QKᵀ computation, scaling, stable Softmax, and the final multiply with V into a single monolithic kernel. Load input blocks once, keep temporaries in SRAM, and write only final outputs.\n",
        "\n",
        "**Pillar 2 — Online (block-by-block) Softmax**\n",
        "Process scores in tiles while maintaining running statistics (running maximum and running sum) to perform numerically stable Softmax incrementally. This avoids needing the full row in SRAM at once.\n",
        "\n",
        "**Pillar 3 — Recomputation in backpropagation**\n",
        "In the backward pass, recompute $S$ (and related intermediates) on the fly from $Q,K,V$ rather than reading huge saved matrices from HBM. Recompute is faster than fetching gigabytes from off-chip memory.\n",
        "\n",
        "These pillars reduce I/O complexity from $O(N^2)$ to effectively $O(N)$ extra HBM accesses while preserving exactness.\n",
        "\n",
        "---\n",
        "\n",
        "## Chapter 5 — FlashAttention’s Memory Structure\n",
        "\n",
        "### 5.1 Chunk / Tile Dimensions (example)\n",
        "\n",
        "Using $B = 128$ tokens and $d = 64$:\n",
        "\n",
        "* Q block: $B\\times d = 128 \\times 64$ → ~32 KB (4-byte floats).\n",
        "* K block: same as Q block → ~32 KB.\n",
        "* V block: same → ~32 KB.\n",
        "* Score tile (temporary): $B\\times B = 128 \\times 128$ → ~65 KB. **Ephemeral**; created, consumed (Softmax), and discarded — never written to HBM.\n",
        "* Output accumulator: $B\\times d$ → ~32 KB; persists for the row block until final write.\n",
        "\n",
        "Total on-chip scratch per tile: on the order of a few 100 KB (constant w.r.t. sequence length).\n",
        "\n",
        "### 5.2 Why FlashAttention is $O(N)$ Space (practical)\n",
        "\n",
        "At any time FlashAttention keeps only a fixed number of blocks in SRAM (Q, one K, one V, the accumulator, and a score tile). This is a constant amount of scratch memory independent of $N$. The only HBM storage required beyond inputs is the final output $O$, so overall memory usage is $O(N)$ rather than $O(N^2)$.\n",
        "\n",
        "### 5.3 Comparison: Standard vs FlashAttention\n",
        "\n",
        "* **Standard Attention:** HBM must store $Q,K,V$ plus the full $N\\times N$ $S$ and $P$ matrices → $O(N^2)$ memory.\n",
        "* **FlashAttention:** HBM stores only $Q,K,V$ and output $O$. Intermediate tiles remain ephemeral in SRAM → $O(N)$ memory.\n",
        "\n",
        "---\n",
        "\n",
        "## Chapter 6 — Mathematical Proof: Why Accumulation Works\n",
        "\n",
        "### 6.1 Softmax and Output\n",
        "\n",
        "Softmax probabilities:\n",
        "\n",
        "$$\n",
        "P_{ij} = \\frac{e^{S_{ij}}}{\\sum_k e^{S_{ik}}}.\n",
        "$$\n",
        "\n",
        "Output for token $i$:\n",
        "\n",
        "$$\n",
        "O_i = \\sum_j P_{ij},V_j ;=; \\sum_j \\frac{e^{S_{ij}}}{\\sum_k e^{S_{ik}}};V_j.\n",
        "$$\n",
        "\n",
        "### 6.2 Partial / Block Processing Problem\n",
        "\n",
        "Processing only a subset (a block) of $K$ and $V$ yields only partial scores $S_{i,\\text{block}}$. The denominator in Softmax requires the sum over all blocks.\n",
        "\n",
        "### 6.3 Online Softmax with Rescaling (procedure)\n",
        "\n",
        "Process blocks sequentially while maintaining numerically stable running statistics.\n",
        "\n",
        "**Process Block 1:**\n",
        "\n",
        "* Let $m_1 = \\max(\\text{scores in Block 1})$.\n",
        "* Compute unnormalized contributions: $p_{1,j} = e^{S_{ij} - m_1}$.\n",
        "* Sum: $\\ell_1 = \\sum_{j\\in\\text{block1}} p_{1,j}$.\n",
        "* Partial accumulator: $O_{\\text{partial}} = \\sum_{j\\in\\text{block1}} p_{1,j},V_j$ (note: unnormalized).\n",
        "\n",
        "**Process Block 2:**\n",
        "\n",
        "* Let $m_2$ be the max over seen scores (block1 ∪ block2).\n",
        "* The old contributions must be rescaled to the new reference: define $\\alpha = e^{m_1 - m_2}$.\n",
        "* Rescale old normalizer: $\\ell_1' = \\ell_1 \\cdot \\alpha$.\n",
        "* New block normalizer: $\\ell_2 = \\sum_{j\\in\\text{block2}} e^{S_{ij} - m_2}$.\n",
        "* Combined normalizer: $\\ell_{\\text{new}} = \\ell_1' + \\ell_2$.\n",
        "* Update accumulator to maintain $O_{\\text{partial}}$ in the same normalization reference; then add block2 contributions appropriately.\n",
        "\n",
        "Repeat for all blocks.\n",
        "\n",
        "### 6.4 Correctness\n",
        "\n",
        "After all blocks, the final accumulator equals\n",
        "\n",
        "$$\n",
        "O_i = \\frac{\\sum_{j=1}^N e^{S_{ij} - m_{\\text{global}}},V_j}{\\sum_{j=1}^N e^{S_{ij} - m_{\\text{global}}}},\n",
        "$$\n",
        "\n",
        "where $m_{\\text{global}} = \\max_j S_{ij}$. Because Softmax is invariant to additive shifts, this equals the standard Softmax result. Therefore the online accumulation is **exact** (no approximation), and numerically stable (subtracting max prevents overflow).\n",
        "\n",
        "---\n",
        "\n",
        "## Chapter 7 — Numerical Stability: Why We Subtract the Max\n",
        "\n",
        "### 7.1 Overflow Risk\n",
        "\n",
        "Float32 has limited dynamic range (~$10^{38}$). Exponentiating large positive values (e.g., $e^{1000}$) overflows. In attention scores, differences can be large.\n",
        "\n",
        "### 7.2 Stable Softmax by Subtracting the Max\n",
        "\n",
        "Softmax is shift invariant:\n",
        "\n",
        "$$\n",
        "\\frac{e^a}{e^a+e^b} ;=; \\frac{e^{a-m}}{e^{a-m}+e^{b-m}} \\quad\\text{for any } m.\n",
        "$$\n",
        "\n",
        "Practical step: subtract $m_i=\\max_k S_{ik}$ from all scores in row $i$ (or use running $m$ per block). This makes the largest exponent $e^0=1$ and prevents overflow. The online algorithm uses this technique per block and then rescales as needed.\n",
        "\n",
        "---\n",
        "\n",
        "## Chapter 8 — Time Complexity vs Runtime\n",
        "\n",
        "### 8.1 Same FLOPs, different runtime\n",
        "\n",
        "Both standard attention and FlashAttention perform $O(N^2)$ arithmetic operations (FLOPs). The difference is **where time is spent**: arithmetic vs memory traffic.\n",
        "\n",
        "### 8.2 Runtime is dominated by data movement\n",
        "\n",
        "* **Standard attention**: memory-bound — many HBM transfers; compute units idle waiting on data.\n",
        "* **FlashAttention**: compute-heavy on-chip; HBM transfers minimized; higher sustained GPU utilization.\n",
        "\n",
        "### 8.3 Practical physics (illustrative numbers)\n",
        "\n",
        "* Example numbers (illustrative): compute throughput on modern GPUs can be hundreds of TFLOPS, while memory bandwidth is in TB/s. Moving data is often tens to hundreds of times more costly than additional arithmetic on on-chip data. Thus recomputing is frequently cheaper than reloading large intermediates from HBM.\n",
        "\n",
        "**Rule of thumb:** Prefer extra on-chip computation if it avoids off-chip memory traffic.\n",
        "\n",
        "---\n",
        "\n",
        "## Chapter 9 — On-Chip Memory (SRAM) Explained\n",
        "\n",
        "### 9.1 Physical reality\n",
        "\n",
        "* SRAM (shared memory, L1) resides inside the GPU die, very close to ALUs — nanosecond latencies and extremely high bandwidth.\n",
        "* HBM is off-die — longer wires, microsecond-class latencies and lower effective bandwidth.\n",
        "\n",
        "### 9.2 Why SRAM is small\n",
        "\n",
        "Latency and bandwidth scale with physical proximity; to maintain nanosecond access times the on-chip memory must be small. There is a tradeoff: small but very fast SRAM vs large but slower HBM.\n",
        "\n",
        "### 9.3 Cache / memory hierarchy (typical view)\n",
        "\n",
        "| Level                     |   Typical size | Purpose                    |\n",
        "| ------------------------- | -------------: | -------------------------- |\n",
        "| Registers                 |     per thread | fastest per-thread storage |\n",
        "| Shared memory (SRAM / L1) | ~100 KB per SM | programmer-managed scratch |\n",
        "| L2 cache                  |       a few MB | GPU-wide cache             |\n",
        "| HBM (global)              |     tens of GB | main memory                |\n",
        "\n",
        "FlashAttention maximizes use of shared memory to keep intermediates on-chip.\n",
        "\n",
        "---\n",
        "\n",
        "## Chapter 10 — Summary Table\n",
        "\n",
        "| Aspect                  |     Standard Attention |                  FlashAttention |\n",
        "| ----------------------- | ---------------------: | ------------------------------: |\n",
        "| Time complexity (FLOPs) |               $O(N^2)$ |                        $O(N^2)$ |\n",
        "| Space complexity        |               $O(N^2)$ |                          $O(N)$ |\n",
        "| I/O complexity          |               $O(N^2)$ | $O(N)$ (effective HBM accesses) |\n",
        "| GPU utilization         |  Low (bandwidth-bound) |            High (compute-bound) |\n",
        "| Intermediates           |          Stored in HBM |               Ephemeral in SRAM |\n",
        "| Softmax strategy        |                 Global |         Online (block-by-block) |\n",
        "| Backward pass           |    Read saved matrices |            Recompute on the fly |\n",
        "| Result accuracy         |                  Exact |                           Exact |\n",
        "| Best use case           | Short sequences (< 1k) |           Long sequences (≫ 1k) |\n",
        "\n",
        "---\n",
        "\n",
        "## Chapter 11 — Final Takeaways\n",
        "\n",
        "1. **FlashAttention is exact.** It computes the same attention result as standard attention.\n",
        "2. **Bandwidth is the bottleneck, not arithmetic.** Data movement dominates runtime on modern GPUs.\n",
        "3. **Online Softmax + kernel fusion + recomputation** enable exact, memory-efficient attention.\n",
        "4. **Recomputation is often cheaper than reloading from HBM.** Use arithmetic to avoid off-chip traffic.\n",
        "5. **Hardware-aware algorithm design wins.** Algorithms that are aware of SRAM vs HBM constraints deliver real performance.\n",
        "\n",
        "**Next steps:** implement or use a proven FlashAttention implementation (CUDA/Triton), profile your model with/without it, and measure context window scaling.\n",
        "\n",
        "---\n",
        "\n",
        "## Appendix — Key Formulas at a Glance\n",
        "\n",
        "**Standard Softmax (row $i$):**\n",
        "\n",
        "$$\n",
        "O_i ;=; \\sum_{j=1}^N \\frac{\\exp(S_{ij})}{\\sum_{k=1}^N \\exp(S_{ik})};V_j\n",
        "$$\n",
        "\n",
        "**Numerically stable Softmax (subtract max):**\n",
        "\n",
        "$$\n",
        "O_i ;=; \\sum_{j=1}^N \\frac{\\exp(S_{ij} - m_i)}{\\sum_{k=1}^N \\exp(S_{ik} - m_i)};V_j,\n",
        "\\qquad\n",
        "m_i = \\max_k S_{ik}.\n",
        "$$\n",
        "\n",
        "**FlashAttention — blockwise (online) accumulation:**\n",
        "\n",
        "When combining an old partial accumulator with a new block:\n",
        "\n",
        "$$\n",
        "O_i \\leftarrow\n",
        "\\frac{\\ell_{\\text{old}};O_i ;+; \\displaystyle\\sum_{j\\in\\text{new block}} e^{S_{ij}-m_{\\text{new}}},V_j}\n",
        "{\\ell_{\\text{old}} + \\ell_{\\text{new}}},\n",
        "$$\n",
        "\n",
        "where\n",
        "\n",
        "$$\n",
        "\\ell_{\\text{old}} = \\sum_{j\\in\\text{old blocks}} e^{S_{ij}-m_{\\text{old}}}, \\quad\n",
        "\\ell_{\\text{new}} = \\sum_{j\\in\\text{new block}} e^{S_{ij}-m_{\\text{new}}}.\n",
        "$$\n",
        "\n",
        "This yields the correct final Softmax weighted sum after all blocks are processed.\n",
        "\n",
        "**I/O complexity (sketch):**\n",
        "\n",
        "$$\n",
        "\\Theta!\\left(\\frac{N^2 d^2}{M}\\right)\n",
        "$$\n",
        "\n",
        "where $M$ denotes available on-chip scratch (in elements). For reasonable tile sizes this simplifies to effectively $O(N)$ additional HBM accesses.\n",
        "\n",
        "---\n",
        "\n",
        "**End of Booklet**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "wLpi3lKp5EQt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fN4WqL9A4_gy"
      },
      "outputs": [],
      "source": []
    }
  ]
}